{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yoom618/TSLib\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "parent_dir = os.path.dirname(os.path.abspath('./'))\n",
    "os.chdir(parent_dir)\n",
    "print(parent_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           ETTh1_96_96         Model:              TimesNet            \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               ETTh1               Root Path:          ./dataset/ETT-small/\n",
      "  Data Path:          ETTh1.csv           Features:           M                   \n",
      "  Target:             OT                  Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            96                  Label Len:          48                  \n",
      "  Pred Len:           96                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             7                   Dec In:             7                   \n",
      "  C Out:              7                   d model:            16                  \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               32                  \n",
      "  Moving Avg:         25                  Factor:             3                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       10                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                Exp                 Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      1                   Devices:            0,1                 \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_ETTh1_96_96_TimesNet_ETTh1_ftM_sl96_ll48_pl96_dm16_nh8_el2_dl1_df32_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 8449\n",
      "val 2785\n",
      "test 2785\n",
      "\titers: 100, epoch: 1 | loss: 0.4681848\n",
      "\tspeed: 0.2032s/iter; left time: 518.4607s\n",
      "\titers: 200, epoch: 1 | loss: 0.4499942\n",
      "\tspeed: 0.1876s/iter; left time: 459.7548s\n",
      "Epoch: 1 cost time: 51.17254304885864\n",
      "Epoch: 1, Steps: 265 | Train Loss: 0.4937456 Vali Loss: 0.8372725 Test Loss: 0.4338118\n",
      "Validation loss decreased (inf --> 0.837272).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.4131877\n",
      "\tspeed: 0.4844s/iter; left time: 1107.3830s\n",
      "\titers: 200, epoch: 2 | loss: 0.3628313\n",
      "\tspeed: 0.1823s/iter; left time: 398.4041s\n",
      "Epoch: 2 cost time: 47.76224875450134\n",
      "Epoch: 2, Steps: 265 | Train Loss: 0.3983794 Vali Loss: 0.7989088 Test Loss: 0.3959864\n",
      "Validation loss decreased (0.837272 --> 0.798909).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.3457576\n",
      "\tspeed: 0.4594s/iter; left time: 928.5125s\n",
      "\titers: 200, epoch: 3 | loss: 0.3333201\n",
      "\tspeed: 0.1779s/iter; left time: 341.8269s\n",
      "Epoch: 3 cost time: 47.49016332626343\n",
      "Epoch: 3, Steps: 265 | Train Loss: 0.3820597 Vali Loss: 0.7969272 Test Loss: 0.3913118\n",
      "Validation loss decreased (0.798909 --> 0.796927).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.3833814\n",
      "\tspeed: 0.4590s/iter; left time: 806.0271s\n",
      "\titers: 200, epoch: 4 | loss: 0.3935638\n",
      "\tspeed: 0.1748s/iter; left time: 289.5014s\n",
      "Epoch: 4 cost time: 46.6230742931366\n",
      "Epoch: 4, Steps: 265 | Train Loss: 0.3705249 Vali Loss: 0.7776363 Test Loss: 0.3904526\n",
      "Validation loss decreased (0.796927 --> 0.777636).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.3564982\n",
      "\tspeed: 0.4702s/iter; left time: 701.1004s\n",
      "\titers: 200, epoch: 5 | loss: 0.3758830\n",
      "\tspeed: 0.1791s/iter; left time: 249.0681s\n",
      "Epoch: 5 cost time: 47.79824662208557\n",
      "Epoch: 5, Steps: 265 | Train Loss: 0.3673547 Vali Loss: 0.7740959 Test Loss: 0.3899511\n",
      "Validation loss decreased (0.777636 --> 0.774096).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.3723269\n",
      "\tspeed: 0.4818s/iter; left time: 590.6470s\n",
      "\titers: 200, epoch: 6 | loss: 0.3763078\n",
      "\tspeed: 0.1772s/iter; left time: 199.4945s\n",
      "Epoch: 6 cost time: 47.38227605819702\n",
      "Epoch: 6, Steps: 265 | Train Loss: 0.3656590 Vali Loss: 0.7785391 Test Loss: 0.3899121\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.4122493\n",
      "\tspeed: 0.4701s/iter; left time: 451.7558s\n",
      "\titers: 200, epoch: 7 | loss: 0.3211073\n",
      "\tspeed: 0.1787s/iter; left time: 153.8572s\n",
      "Epoch: 7 cost time: 47.05767560005188\n",
      "Epoch: 7, Steps: 265 | Train Loss: 0.3646663 Vali Loss: 0.7708418 Test Loss: 0.3898411\n",
      "Validation loss decreased (0.774096 --> 0.770842).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.3264705\n",
      "\tspeed: 0.4603s/iter; left time: 320.3831s\n",
      "\titers: 200, epoch: 8 | loss: 0.3191996\n",
      "\tspeed: 0.1662s/iter; left time: 99.0286s\n",
      "Epoch: 8 cost time: 45.45288610458374\n",
      "Epoch: 8, Steps: 265 | Train Loss: 0.3639844 Vali Loss: 0.7734663 Test Loss: 0.3897951\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.3810563\n",
      "\tspeed: 0.4619s/iter; left time: 199.0607s\n",
      "\titers: 200, epoch: 9 | loss: 0.3634709\n",
      "\tspeed: 0.1732s/iter; left time: 57.3347s\n",
      "Epoch: 9 cost time: 46.340617418289185\n",
      "Epoch: 9, Steps: 265 | Train Loss: 0.3646829 Vali Loss: 0.7817264 Test Loss: 0.3897694\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.3844203\n",
      "\tspeed: 0.4744s/iter; left time: 78.7478s\n",
      "\titers: 200, epoch: 10 | loss: 0.4601144\n",
      "\tspeed: 0.1716s/iter; left time: 11.3285s\n",
      "Epoch: 10 cost time: 46.02703905105591\n",
      "Epoch: 10, Steps: 265 | Train Loss: 0.3637295 Vali Loss: 0.7705921 Test Loss: 0.3897573\n",
      "Validation loss decreased (0.770842 --> 0.770592).  Saving model ...\n",
      "Updating learning rate to 1.953125e-07\n",
      ">>>>>>>testing : long_term_forecast_ETTh1_96_96_TimesNet_ETTh1_ftM_sl96_ll48_pl96_dm16_nh8_el2_dl1_df32_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 2785\n",
      "test shape: (2785, 96, 7) (2785, 96, 7)\n",
      "test shape: (2785, 96, 7) (2785, 96, 7)\n",
      "mse:0.3892529606819153, mae:0.4121544063091278, dtw:Not calculated\n",
      "Using GPU\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           ETTh1_96_192        Model:              TimesNet            \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               ETTh1               Root Path:          ./dataset/ETT-small/\n",
      "  Data Path:          ETTh1.csv           Features:           M                   \n",
      "  Target:             OT                  Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            96                  Label Len:          48                  \n",
      "  Pred Len:           192                 Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             7                   Dec In:             7                   \n",
      "  C Out:              7                   d model:            16                  \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               32                  \n",
      "  Moving Avg:         25                  Factor:             3                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       10                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                Exp                 Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      1                   Devices:            0,1                 \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_ETTh1_96_192_TimesNet_ETTh1_ftM_sl96_ll48_pl192_dm16_nh8_el2_dl1_df32_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 8353\n",
      "val 2689\n",
      "test 2689\n",
      "\titers: 100, epoch: 1 | loss: 0.6533690\n",
      "\tspeed: 0.2300s/iter; left time: 579.8869s\n",
      "\titers: 200, epoch: 1 | loss: 0.4659614\n",
      "\tspeed: 0.2024s/iter; left time: 489.9860s\n",
      "Epoch: 1 cost time: 56.370238065719604\n",
      "Epoch: 1, Steps: 262 | Train Loss: 0.5500580 Vali Loss: 1.1594229 Test Loss: 0.4977426\n",
      "Validation loss decreased (inf --> 1.159423).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.4533339\n",
      "\tspeed: 0.5530s/iter; left time: 1249.3361s\n",
      "\titers: 200, epoch: 2 | loss: 0.4782013\n",
      "\tspeed: 0.2050s/iter; left time: 442.5089s\n",
      "Epoch: 2 cost time: 54.20211100578308\n",
      "Epoch: 2, Steps: 262 | Train Loss: 0.4512162 Vali Loss: 1.0792822 Test Loss: 0.4451043\n",
      "Validation loss decreased (1.159423 --> 1.079282).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.4211786\n",
      "\tspeed: 0.5507s/iter; left time: 1099.7441s\n",
      "\titers: 200, epoch: 3 | loss: 0.4352419\n",
      "\tspeed: 0.2038s/iter; left time: 386.6464s\n",
      "Epoch: 3 cost time: 53.71903157234192\n",
      "Epoch: 3, Steps: 262 | Train Loss: 0.4295898 Vali Loss: 1.0484356 Test Loss: 0.4417296\n",
      "Validation loss decreased (1.079282 --> 1.048436).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.4468644\n",
      "\tspeed: 0.5394s/iter; left time: 935.8730s\n",
      "\titers: 200, epoch: 4 | loss: 0.4028620\n",
      "\tspeed: 0.2046s/iter; left time: 334.5426s\n",
      "Epoch: 4 cost time: 53.877312421798706\n",
      "Epoch: 4, Steps: 262 | Train Loss: 0.4188759 Vali Loss: 1.0459487 Test Loss: 0.4424896\n",
      "Validation loss decreased (1.048436 --> 1.045949).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.4217989\n",
      "\tspeed: 0.5605s/iter; left time: 825.5641s\n",
      "\titers: 200, epoch: 5 | loss: 0.3896372\n",
      "\tspeed: 0.1976s/iter; left time: 271.3415s\n",
      "Epoch: 5 cost time: 53.33982872962952\n",
      "Epoch: 5, Steps: 262 | Train Loss: 0.4140882 Vali Loss: 1.0398973 Test Loss: 0.4416366\n",
      "Validation loss decreased (1.045949 --> 1.039897).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.4082997\n",
      "\tspeed: 0.5413s/iter; left time: 655.4919s\n",
      "\titers: 200, epoch: 6 | loss: 0.4031650\n",
      "\tspeed: 0.2040s/iter; left time: 226.6624s\n",
      "Epoch: 6 cost time: 52.673157930374146\n",
      "Epoch: 6, Steps: 262 | Train Loss: 0.4128654 Vali Loss: 1.0410877 Test Loss: 0.4418703\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.4470021\n",
      "\tspeed: 0.5491s/iter; left time: 521.1190s\n",
      "\titers: 200, epoch: 7 | loss: 0.3377989\n",
      "\tspeed: 0.2067s/iter; left time: 175.5236s\n",
      "Epoch: 7 cost time: 54.34511923789978\n",
      "Epoch: 7, Steps: 262 | Train Loss: 0.4111204 Vali Loss: 1.0454565 Test Loss: 0.4417682\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.4407472\n",
      "\tspeed: 0.5466s/iter; left time: 375.5475s\n",
      "\titers: 200, epoch: 8 | loss: 0.3206184\n",
      "\tspeed: 0.2007s/iter; left time: 117.8013s\n",
      "Epoch: 8 cost time: 52.245237588882446\n",
      "Epoch: 8, Steps: 262 | Train Loss: 0.4097077 Vali Loss: 1.0419025 Test Loss: 0.4420001\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_ETTh1_96_192_TimesNet_ETTh1_ftM_sl96_ll48_pl192_dm16_nh8_el2_dl1_df32_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 2689\n",
      "test shape: (2689, 192, 7) (2689, 192, 7)\n",
      "test shape: (2689, 192, 7) (2689, 192, 7)\n",
      "mse:0.4400743544101715, mae:0.4414282739162445, dtw:Not calculated\n",
      "Using GPU\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           ETTh1_96_336        Model:              TimesNet            \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               ETTh1               Root Path:          ./dataset/ETT-small/\n",
      "  Data Path:          ETTh1.csv           Features:           M                   \n",
      "  Target:             OT                  Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            96                  Label Len:          48                  \n",
      "  Pred Len:           336                 Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             7                   Dec In:             7                   \n",
      "  C Out:              7                   d model:            16                  \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               32                  \n",
      "  Moving Avg:         25                  Factor:             3                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       10                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                Exp                 Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      1                   Devices:            0,1                 \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_ETTh1_96_336_TimesNet_ETTh1_ftM_sl96_ll48_pl336_dm16_nh8_el2_dl1_df32_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 8209\n",
      "val 2545\n",
      "test 2545\n",
      "\titers: 100, epoch: 1 | loss: 0.6581647\n",
      "\tspeed: 0.2486s/iter; left time: 614.2852s\n",
      "\titers: 200, epoch: 1 | loss: 0.5836204\n",
      "\tspeed: 0.2305s/iter; left time: 546.6028s\n",
      "Epoch: 1 cost time: 60.508947134017944\n",
      "Epoch: 1, Steps: 257 | Train Loss: 0.6266983 Vali Loss: 1.3884463 Test Loss: 0.5265951\n",
      "Validation loss decreased (inf --> 1.388446).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.5302802\n",
      "\tspeed: 0.5625s/iter; left time: 1245.3037s\n",
      "\titers: 200, epoch: 2 | loss: 0.5986145\n",
      "\tspeed: 0.2114s/iter; left time: 446.9906s\n",
      "Epoch: 2 cost time: 53.963924407958984\n",
      "Epoch: 2, Steps: 257 | Train Loss: 0.5141312 Vali Loss: 1.3846843 Test Loss: 0.4968134\n",
      "Validation loss decreased (1.388446 --> 1.384684).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.4789348\n",
      "\tspeed: 0.5411s/iter; left time: 1058.9830s\n",
      "\titers: 200, epoch: 3 | loss: 0.4518856\n",
      "\tspeed: 0.2140s/iter; left time: 397.3623s\n",
      "Epoch: 3 cost time: 54.086886167526245\n",
      "Epoch: 3, Steps: 257 | Train Loss: 0.4994772 Vali Loss: 1.3867922 Test Loss: 0.4890413\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.5269818\n",
      "\tspeed: 0.5127s/iter; left time: 871.6095s\n",
      "\titers: 200, epoch: 4 | loss: 0.4807198\n",
      "\tspeed: 0.2026s/iter; left time: 324.1998s\n",
      "Epoch: 4 cost time: 52.146761417388916\n",
      "Epoch: 4, Steps: 257 | Train Loss: 0.4909427 Vali Loss: 1.3913897 Test Loss: 0.4876869\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.5274000\n",
      "\tspeed: 0.5093s/iter; left time: 734.9227s\n",
      "\titers: 200, epoch: 5 | loss: 0.4762965\n",
      "\tspeed: 0.1888s/iter; left time: 253.5684s\n",
      "Epoch: 5 cost time: 50.24583315849304\n",
      "Epoch: 5, Steps: 257 | Train Loss: 0.4866082 Vali Loss: 1.3961763 Test Loss: 0.4859414\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_ETTh1_96_336_TimesNet_ETTh1_ftM_sl96_ll48_pl336_dm16_nh8_el2_dl1_df32_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 2545\n",
      "test shape: (2545, 336, 7) (2545, 336, 7)\n",
      "test shape: (2545, 336, 7) (2545, 336, 7)\n",
      "mse:0.495315283536911, mae:0.4714498817920685, dtw:Not calculated\n",
      "Using GPU\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           ETTh1_96_720        Model:              TimesNet            \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               ETTh1               Root Path:          ./dataset/ETT-small/\n",
      "  Data Path:          ETTh1.csv           Features:           M                   \n",
      "  Target:             OT                  Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            96                  Label Len:          48                  \n",
      "  Pred Len:           720                 Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             7                   Dec In:             7                   \n",
      "  C Out:              7                   d model:            16                  \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               32                  \n",
      "  Moving Avg:         25                  Factor:             3                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       10                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                Exp                 Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      1                   Devices:            0,1                 \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_ETTh1_96_720_TimesNet_ETTh1_ftM_sl96_ll48_pl720_dm16_nh8_el2_dl1_df32_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 7825\n",
      "val 2161\n",
      "test 2161\n",
      "\titers: 100, epoch: 1 | loss: 0.7939653\n",
      "\tspeed: 0.2331s/iter; left time: 548.1257s\n",
      "\titers: 200, epoch: 1 | loss: 0.5652310\n",
      "\tspeed: 0.2151s/iter; left time: 484.1586s\n",
      "Epoch: 1 cost time: 54.21904730796814\n",
      "Epoch: 1, Steps: 245 | Train Loss: 0.7296920 Vali Loss: 1.6337390 Test Loss: 0.5408579\n",
      "Validation loss decreased (inf --> 1.633739).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.6678249\n",
      "\tspeed: 0.4910s/iter; left time: 1033.9616s\n",
      "\titers: 200, epoch: 2 | loss: 0.6200081\n",
      "\tspeed: 0.2132s/iter; left time: 427.7395s\n",
      "Epoch: 2 cost time: 51.31041407585144\n",
      "Epoch: 2, Steps: 245 | Train Loss: 0.6228405 Vali Loss: 1.6189721 Test Loss: 0.5190885\n",
      "Validation loss decreased (1.633739 --> 1.618972).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.6479176\n",
      "\tspeed: 0.5446s/iter; left time: 1013.5699s\n",
      "\titers: 200, epoch: 3 | loss: 0.5854300\n",
      "\tspeed: 0.2275s/iter; left time: 400.6663s\n",
      "Epoch: 3 cost time: 56.71495842933655\n",
      "Epoch: 3, Steps: 245 | Train Loss: 0.6041974 Vali Loss: 1.6465762 Test Loss: 0.5208850\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.5605142\n",
      "\tspeed: 0.5382s/iter; left time: 869.6923s\n",
      "\titers: 200, epoch: 4 | loss: 0.5325293\n",
      "\tspeed: 0.2358s/iter; left time: 357.4954s\n",
      "Epoch: 4 cost time: 57.94391465187073\n",
      "Epoch: 4, Steps: 245 | Train Loss: 0.5952385 Vali Loss: 1.6568637 Test Loss: 0.5206170\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.5346146\n",
      "\tspeed: 0.5268s/iter; left time: 722.1774s\n",
      "\titers: 200, epoch: 5 | loss: 0.5950404\n",
      "\tspeed: 0.2244s/iter; left time: 285.1867s\n",
      "Epoch: 5 cost time: 56.109538078308105\n",
      "Epoch: 5, Steps: 245 | Train Loss: 0.5914391 Vali Loss: 1.6620241 Test Loss: 0.5226433\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_ETTh1_96_720_TimesNet_ETTh1_ftM_sl96_ll48_pl720_dm16_nh8_el2_dl1_df32_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 2161\n",
      "test shape: (2161, 720, 7) (2161, 720, 7)\n",
      "test shape: (2161, 720, 7) (2161, 720, 7)\n",
      "mse:0.5179636478424072, mae:0.4939846396446228, dtw:Not calculated\n"
     ]
    }
   ],
   "source": [
    "# long-term forecast\n",
    "!bash ./scripts_custom/long_term_forecast/ETT_script/TimesNet_ETTh1.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# short-term forecast\n",
    "!bash ./scripts_custom/short_term_forecast/TimesNet_M4.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputation\n",
    "!bash ./scripts_custom/imputation/ETT_script/TimesNet_ETTh1.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anomaly detection\n",
    "!bash ./scripts_custom/anomaly_detection/PSM/TimesNet.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification\n",
    "!bash ./scripts_custom/classification/TimesNet.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한꺼번에 돌리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mkdir -p results/short_term_forecast\n",
      "nohup bash -c 'for i in ./scripts_custom/short_term_forecast/*.sh; do\n",
      "echo $i;\n",
      "nohup bash $i > \"results/short_term_forecast/$(basename $i).log\";\n",
      "done' > nohup_short_term_forecast.log\n",
      "\n",
      "\n",
      "mkdir -p results/classification\n",
      "nohup bash -c 'for i in ./scripts_custom/classification/*.sh; do\n",
      "echo $i;\n",
      "nohup bash $i > \"results/classification/$(basename $i).log\";\n",
      "done' > nohup_classification.log\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for task in ['short_term_forecast', 'classification']:\n",
    "    print(f'''\n",
    "mkdir -p results/{task}\n",
    "nohup bash -c 'for i in ./scripts_custom/{task}/*.sh; do\n",
    "echo $i;\n",
    "nohup bash $i > \"results/{task}/$(basename $i).log\";\n",
    "done' > nohup_{task}.log\n",
    "''')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mkdir -p results/anomaly_detection\n",
      "nohup bash -c 'for i in ./scripts_custom/anomaly_detection/*/*.sh; do\n",
      "echo $i;\n",
      "nohup bash $i > \"results/anomaly_detection/$(basename $i).log\";\n",
      "done' > nohup_anomaly_detection.log\n",
      "\n",
      "\n",
      "mkdir -p results/imputation\n",
      "nohup bash -c 'for i in ./scripts_custom/imputation/*/*.sh; do\n",
      "echo $i;\n",
      "nohup bash $i > \"results/imputation/$(basename $i).log\";\n",
      "done' > nohup_imputation.log\n",
      "\n",
      "\n",
      "mkdir -p results/long_term_forecast\n",
      "nohup bash -c 'for i in ./scripts_custom/long_term_forecast/*/*.sh; do\n",
      "echo $i;\n",
      "nohup bash $i > \"results/long_term_forecast/$(basename $i).log\";\n",
      "done' > nohup_long_term_forecast.log\n",
      "\n",
      "\n",
      "mkdir -p results/exogenous_forecast\n",
      "nohup bash -c 'for i in ./scripts_custom/exogenous_forecast/*/*.sh; do\n",
      "echo $i;\n",
      "nohup bash $i > \"results/exogenous_forecast/$(basename $i).log\";\n",
      "done' > nohup_exogenous_forecast.log\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for task in ['anomaly_detection', 'imputation', 'long_term_forecast', 'exogenous_forecast']:\n",
    "    print(f'''\n",
    "mkdir -p results/{task}\n",
    "nohup bash -c 'for i in ./scripts_custom/{task}/*/*.sh; do\n",
    "echo $i;\n",
    "nohup bash $i > \"results/{task}/$(basename $i).log\";\n",
    "done' > nohup_{task}.log\n",
    "''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tslib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
